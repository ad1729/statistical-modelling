---
html_document:
  highlight: monochrome
  keep_md: yes
  theme: journal
  toc: no
  toc_float: yes
  toc_depth: 2
author: "Akshat Dwivedi"
date: '`r date()`'
title: "Assignment 1"
pdf_document:
  highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE)
#If you run into problems with cached output you can always clear the knitr cache by removing the folder named with a _cache suffix within your documentâ€™s directory.
```

```{r include=FALSE}
library(faraway)
library(ggplot2)
library(dplyr)

motor = motorins
```

# Question 1

As an initial step, we fit a GLM model to the motorins data from the package `faraway` with `perd` (payment per claim) as the response variable, all the covariates (except payment and claim since perd is a ratio of these two variables), and assuming a Gamma distribution for the response variable with the *inverse* link function as an initial step. The inverse link function is the default link function for Gamma regression models. The table below shows the (spearman) correlation between the continuous variables in the motorins data. This was chosen over the pearson correlation due to the presence of extreme outliers and rejection of the multivariate normality assumption for pairs of variables using Mardia's Multivariate Normality Test.

```{r}
cor(motor[,5:7], method = "spearman")
```

This shows that the variables are highly correlated with each other indicating multicollinearity. Including claims, payment and insurance in the model together will lead to inflated std errors of the estimated regression coefficients. This can be measured by looking at the next two tables which show VIF and the square root of the VIF which shows how high the standard errors of the coefficients are compared to a model with uncorrelated predictors. 

```{r}
diag(solve(cor(motor[,c(5,6,7)], method = "spearman"))) # gives us the VIF
```

And the square root given by

```{r}
sqrt(diag(solve(cor(motor[,c(5,6,7)], method = "spearman")))) # gives us the VIF
```

Based on this, we drop Claims from the models that will be fit to the data and decide to keep payment in the model.

```{r}
fit1 = glm(perd ~ ., family = Gamma(link = "inverse"), data = motor[,-6])
#summary(fit1)
```

Since we are not interested in *stargazing*, we omit the output from the model fit and report that the AIC value for the fitted model is given as `r AIC(fit1)`. Next, we select the covariates to be included in the model using the stepwise selection approach combined with Akaike Information Criterion. The following table shows the output from running stepwise selection on the fitted model.

```{r}
MASS::stepAIC(fit1, k = 2, scope = list(upper = ~., lower = ~1) , direction = "both")
```

This shows that dropping Kilometres or Bonus from the main effects model leads to the lowest AIC values of 33825. However, dropping Kilometres from the model in the next step results in a larger AIC value and we decide to keep it in the model. 

## (b)
Next, we fit a Gamma model with the log link function.

## (c)

## (d)

# Question 2



# Question 3

**Included separately, attached at the end of this document.**

# Question 4

This question comprises implementing the ridge regression estimator $\hat{\beta}_R = (X^t X + \lambda I_p)^{-1} X^t Y$ in R and using this implementation to estimate the coefficients of the following quadratic additive model $$dist = \beta_0 + \beta_1 * speed + \beta_2 * speed^2 + \varepsilon$$ and plotting the estimated values of $\beta_2$ vs different values of $\lambda$. The `cars` dataset included in R contains the variables `speed` and `distance`. The funtion that implements the ridge estimator in code is given below.

```{r, echo = TRUE}
ridge = function(x, y, lambda) {
  
  # x : design matrix
  # y : response variable
  # lambda : scalar; lambda parameter in the ridge estimator
  # \beta = [(X^t * X + \lambda * I_p) ^ -1] * (X^t * Y)
  # p = ncol(x) in the above statement
  
  xt_x = t(x) %*% as.matrix(x)
  lambda_matrix = lambda * diag(x = 1, nrow = ncol(x))
  beta_ridge = solve(xt_x + lambda_matrix) %*% (t(x) %*% y) %>% t() %>% data.frame()
  return(beta_ridge)
}
```

We evaluate the coefficients for $\lambda \in \{0,1,2,3,\ldots,100\}$, i.e. for 100 equally spaced values of lambda starting from 0 and ending on 100. Furthermore, the variables are centered and scaled so the intercept term does not need to be estimated and can be dropped from the model. Thus, only coefficients $\beta_1$ and $\beta_2$ are estimated for the model. The next figure contains the a plot of the $\beta_2$ vs $\lambda$.

```{r}
x = scale(cars$speed)
y = scale(cars$dist)
lambda = seq(from = 0, to = 100, length = 100)
design_matrix = data.frame(speed = x, speed2 = x ^ 2)

## getting the estimated coefficient values for different lambda values between 0 and 1
estimated_coef = data.frame()

for (i in 1:length(lambda)) {
  estimated_coef = rbind(estimated_coef, ridge(design_matrix, y, lambda[i]))
}

ggplot(data = estimated_coef, aes(x = lambda, y = speed2)) + geom_line() + xlab("lambda") + ylab("Coefficient of Quadratic Effect") + ggtitle("Plot of beta_2 vs lambda") + theme_bw()
```

From the plot above, we see that as the value of $\lambda$ increases, the shrinkage of the coefficient(s) tends towards zero, i.e., the coefficients get smaller and smaller. For example, for $\lambda = 0$, $\hat{\beta}_2 \approx 0.06$ whereas $\hat{\beta}_2 = 0.02$ for $\lambda = 100$.

# Appendix (R Code)
```{r, eval = FALSE, echo = TRUE}

```

